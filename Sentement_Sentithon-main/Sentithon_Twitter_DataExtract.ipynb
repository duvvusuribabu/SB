{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/XGPm9Dhn/cLwR3z2YmH5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BALU2000-creator/Sentement_Sentithon/blob/main/Sentithon_Twitter_DataExtract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snscrape"
      ],
      "metadata": {
        "id": "JTQeJxYyNYbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "YsKCSoVuNzsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yake"
      ],
      "metadata": {
        "id": "1WjV0jrCNVZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!!pip install langdetect"
      ],
      "metadata": {
        "id": "v0mSH7unPEEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clean-text"
      ],
      "metadata": {
        "id": "cbfspOikPTHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from googletrans import Translator\n",
        "from langdetect import detect\n",
        "import re\n",
        "from cleantext import clean\n",
        "import yake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D52-zU2vNkgW",
        "outputId": "ff54b9eb-51ba-4c24-b86e-2a02824a3f74"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XlQ1oNifNKWT"
      },
      "outputs": [],
      "source": [
        "class keywords_extract():\n",
        "  def get_key_words(self, text):\n",
        "    kw_extractor = yake.KeywordExtractor()\n",
        "    to_be_search = [text]                                      \n",
        "    language = \"en\"\n",
        "    max_ngram_size = 3\n",
        "    deduplication_threshold = 0.9\n",
        "    numOfKeywords = 20\n",
        "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
        "    keywords = custom_kw_extractor.extract_keywords(text)\n",
        "    for kw in keywords:\n",
        "      to_be_search.append(kw[0])\n",
        "    return to_be_search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator()\n",
        "\n",
        "def pre_processor(text):\n",
        "  text = re.sub(r\"\\s*@\\w*\\s*\", \" \", text).strip()\n",
        "  text = re.sub(r\"http\\S+|http[s]?://\\S+|http://\\S+|https://\\S+\", \"\", text)\n",
        "  text = re.sub(r\"[#\\n\\t!]\",'', text)\n",
        "  return text\n",
        "text = \"@desimojito !Yea#h Morbi bridge collapse, rampant corruption, 2nd wave COVID mismanagement development ðŸ””\\n\\nâ€˜Bus Abdul ka screhttps://www.java2blog.comw tight karna haiâ€™ is what they voted for. ðŸ”” atmanirbhar ðŸ˜‚ðŸ¤£ðŸ˜‚',\"\n",
        "pre_processor(text)\n",
        "\n",
        "def lang_process(text):\n",
        "  text = pre_processor(text)\n",
        "  # print('lang_process- text', text)\n",
        "  lan = detect(text)\n",
        "  # print('lannnnn')\n",
        "  if lan!='en':\n",
        "    translated_obj = translator.translate(text, dest='en')  \n",
        "    text = translated_obj.text\n",
        "  return text, lan\n",
        "\n"
      ],
      "metadata": {
        "id": "z6eLh9exNSLC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Process_Twitter():\n",
        "\n",
        "  def __init__(self, topic='Atmanirbhar Bharat: Local Vocalist', selection = 'Public', since='2021-11-01', untill='2022-12-09', maximum_tweets=300):\n",
        "    keywords_extractor = keywords_extract()\n",
        "    self.to_be_search = keywords_extractor.get_key_words(topic)\n",
        "    self.len_of_df = maximum_tweets\n",
        "    print(self.to_be_search)\n",
        "    self.since = since\n",
        "    self.untill = untill\n",
        "    self.total_langs = []\n",
        "\n",
        "    self.top_journalists = [('anjana om kashyap','anjanaomkashyap'),   #Top journalists and Their Twitter username\n",
        "                   ('abhisar sharma','abhisar_sharma'),\n",
        "                   ('vikram chandra','vikramchandra'),\n",
        "                   ('Sudhir Chaudhary','sudhirchaudhary'), \n",
        "                   ('swetha singh','zoo_bear'),\n",
        "                   ('rajat sharma','RajatSharmaLive'), \n",
        "                   ('nidhi razdan','Nidhi'), \n",
        "                   ('Rajdeep Sardesai','sardesairajdeep'),\n",
        "                   ('Annap Goswami','AnnapGoswami'), \n",
        "                   ('Ravish Kumar', 'ravishndtv'),\n",
        "                   ('Narendra Modi', 'narendramodi')]\n",
        "    self.reference_tweet_attributes = ['tweet.date', 'tweet.id', 'tweet.content', 'tweet.url',\n",
        "                          'tweet.user.username', 'tweet.user.followersCount','tweet.replyCount',\n",
        "                          'tweet.retweetCount', 'tweet.likeCount', 'tweet.quoteCount', 'tweet.lang',\n",
        "                          'tweet.outlinks', 'tweet.media', 'tweet.retweetedTweet', 'tweet.quotedTweet',\n",
        "                          'tweet.inReplyToTweetId', 'tweet.inReplyToUser', 'tweet.mentionedUsers',\n",
        "                          'tweet.coordinates', 'tweet.place', 'tweet.hashtags', 'tweet.cashtags']\n",
        "    self.of_public_df  = None\n",
        "    self.of_journals_df= None\n",
        "    if selection=='Both':\n",
        "      self.of_public_df = self.create_public()\n",
        "      self.of_journals_df = self.create_journalists()\n",
        "    elif selection == 'Journalists':\n",
        "      self.of_public_df = self.create_journalists()\n",
        "    else:\n",
        "      self.of_public_df = self.create_public()\n",
        "\n",
        "   \n",
        "  def create_public(self):\n",
        "    attributes_container = []\n",
        "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "    for search in tqdm(self.to_be_search):\n",
        "      print('search.....', search)\n",
        "      for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{search} since:2021-11-01 until:2022-12-09').get_items()):\n",
        "          text = tweet.content\n",
        "          try:\n",
        "            text = pre_processor(text)\n",
        "            lan = detect(text)\n",
        "            if lan!='en':\n",
        "              translated_obj = translator.translate(text, dest='en')  \n",
        "              text = translated_obj.text\n",
        "          except:\n",
        "            lan=None\n",
        "            continue\n",
        "          self.total_langs.append(lan)\n",
        "          tweet.content = text\n",
        "          if len(attributes_container)>self.len_of_df:\n",
        "              break\n",
        "          attributes_container.append([tweet.user.username, tweet.date, tweet.likeCount, tweet.content, tweet.url])\n",
        "      if len(attributes_container)>self.len_of_df:\n",
        "        break\n",
        "    tweets_df = pd.DataFrame(attributes_container, columns=[\"User\", \"Date Created\", \"Number of Likes\", \"Tweet\", \"url\"])\n",
        "    print('create_public is succesful')\n",
        "    del attributes_container\n",
        "    return tweets_df\n",
        "  \n",
        "  def create_journalists(self):\n",
        "    attributes_container_ = []\n",
        "    top_journalists_accounts = []                \n",
        "    for _,account in self.top_journalists:\n",
        "      top_journalists_accounts.append(account)\n",
        "    users_names = top_journalists_accounts\n",
        "    # for n, k in tqdm(enumerate(users_name)):\n",
        "    for user_name in tqdm(users_names):\n",
        "      for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:{}'.format(user_name)).get_items()):\n",
        "        if len(attributes_container_)>self.len_of_df:\n",
        "          break\n",
        "        text = tweet.content\n",
        "        try:\n",
        "          text = pre_processor(text)\n",
        "          lan = detect(text)\n",
        "          if lan!='en':\n",
        "            translated_obj = translator.translate(text, dest='en')  \n",
        "            text = translated_obj.text\n",
        "        except:\n",
        "          lan=None\n",
        "          continue\n",
        "        self.total_langs.append(lan)\n",
        "        tweet.content = text\n",
        "\n",
        "        # for keyword in to_be_search:\n",
        "        #   if keyword in tweet.content:\n",
        "        attributes_container_.append([tweet.user.username, tweet.date, tweet.likeCount, tweet.content, tweet.url])\n",
        "      if len(attributes_container_)>self.len_of_df:\n",
        "        break\n",
        "    tweets_df = pd.DataFrame(attributes_container_, columns=[\"User\", \"Date Created\", \"Number of Likes\", \"Tweet\", \"url\"])\n",
        "    print('create_journalists is succesful')\n",
        "    del attributes_container_\n",
        "    return tweets_df\n",
        "\n",
        "  def get_public_journalists_both(self):\n",
        "    # if self.of_public_df and self.of_journals_df:\n",
        "    total_df = pd.concat([self.of_public_df, self.of_journals_df], axis=0)\n",
        "    return total_df, self.of_public_df, self.of_journals_df\n",
        "    # else:\n",
        "    #   print('some missing in Process_Twitter get_public_journalistsmethod')\n",
        "    \n",
        "  def get_total_languages(self):\n",
        "    return self.total_langs\n",
        "  \n",
        "  def __del__(self):\n",
        "    del total_df\n",
        "    del self.of_public_df\n",
        "    del self.of_journals_df\n",
        "    print('Destructor called, Employee deleted.')"
      ],
      "metadata": {
        "id": "Om-b3K7KNT6t"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import maximum\n",
        "search_key_words = ['Atmanirbhar Bharat: Local Vocalist',\n",
        "'Digital India',\n",
        "'Covid-19 Booster Vaccination Shots',\n",
        "'recent Indian government initiative']\n",
        "total_langs = []\n",
        "for topic in search_key_words:\n",
        "  x = Process_Twitter(topic = topic, selection='Both', maximum_tweets=300)\n",
        "  ALl_dfs = x.get_public_journalists_both()\n",
        "  total_langs.append(x.get_total_languages())\n",
        "  del x\n",
        "  file_names = ('TweetTotal_', 'TweetPublic_', 'TweetJournalists_')\n",
        "  for i in range(3):\n",
        "    df = list(ALl_dfs[i]['Tweet'])\n",
        "    f = open(file_names[i]+topic+'.txt', 'w')\n",
        "    for text in df:\n",
        "      f.write(text)\n",
        "      f.write('\\n')"
      ],
      "metadata": {
        "id": "dQxrdYCPQyu2"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}